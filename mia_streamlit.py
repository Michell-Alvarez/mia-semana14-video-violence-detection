# -------------------------------
# STREAMLIT Y COMPONENTES DE UI
# -------------------------------

# Librer√≠a principal para crear interfaces web interactivas
import streamlit as st

# M√≥dulo adicional para crear men√∫s laterales personalizados
from streamlit_option_menu import option_menu

# -------------------------------
# SISTEMA DE ARCHIVOS Y UTILIDADES
# -------------------------------

# Librer√≠a est√°ndar de Python para manipular rutas y archivos
import os
# Permite la creaci√≥n de archivos temporales
import tempfile
# Permite ejecutar comandos de terminal desde Python
import subprocess
# Permite copiar archivos o directorios
import shutil

# -------------------------------
# VISI√ìN POR COMPUTADOR Y TRANSFORMACIONES
# -------------------------------

# OpenCV para procesamiento de im√°genes y lectura de video
import cv2
# Transformaciones para preprocesar im√°genes (ToTensor, Normalize, etc.)
from torchvision import transforms

# -------------------------------
# PYTORCH (DEEP LEARNING)
# -------------------------------

# Librer√≠a principal para entrenamiento e inferencia con redes neuronales
import torch
# Subm√≥dulo de PyTorch para construir modelos de redes neuronales
import torch.nn as nn



# ==== Modelo ====
# Definici√≥n de una red neuronal convolucional 3D mejorada
class Improved3DCNN(nn.Module):
    # Constructor del modelo
    def __init__(self):
        # Inicializa la clase base nn.Module
        super(Improved3DCNN, self).__init__()

        # Primera capa convolucional 3D: de 3 canales (RGB) a 16 canales
        self.conv1 = nn.Conv3d(3, 16, kernel_size=(3, 3, 3), padding=1)
        self.dropout1 = nn.Dropout(0.2)  # Dropout para reducir overfitting

        # Pooling para reducir las dimensiones espaciales (alto y ancho)
        self.pool = nn.MaxPool3d(kernel_size=(1, 2, 2))

        # Segunda capa convolucional: de 16 a 32 canales
        self.conv2 = nn.Conv3d(16, 32, kernel_size=(3, 3, 3), padding=1)
        self.dropout2 = nn.Dropout(0.2)

        # Tercera capa convolucional: de 32 a 64 canales
        self.conv3 = nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=1)
        self.dropout3 = nn.Dropout(0.2)

        # Calcula autom√°ticamente el tama√±o de entrada de la primera capa fully connected
        self.fc_input_size = self._get_fc_input_size()

        # Capa completamente conectada: reduce a 128 neuronas
        self.fc1 = nn.Linear(self.fc_input_size, 128)
        self.dropout_fc = nn.Dropout(0.2)

        # Capa de salida con 2 neuronas (para clasificaci√≥n binaria)
        self.fc2 = nn.Linear(128, 2)

    # M√©todo auxiliar para calcular el tama√±o que tendr√° el tensor tras las convoluciones y poolings
    def _get_fc_input_size(self):
        with torch.no_grad():  # No requiere gradientes
            # Crea un tensor de prueba con forma [batch, channels, time, height, width]
            x = torch.zeros(1, 3, 30, 224, 224)
            # Aplica las tres convoluciones seguidas de pooling y ReLU
            x = self.pool(nn.functional.relu(self.conv1(x)))
            x = self.pool(nn.functional.relu(self.conv2(x)))
            x = self.pool(nn.functional.relu(self.conv3(x)))
            return x.numel()  # Devuelve el n√∫mero total de elementos

    # M√©todo de propagaci√≥n hacia adelante
    def forward(self, x):
        x = self.pool(nn.functional.relu(self.conv1(x)))
        x = self.dropout1(x)
        x = self.pool(nn.functional.relu(self.conv2(x)))
        x = self.dropout2(x)
        x = self.pool(nn.functional.relu(self.conv3(x)))
        x = self.dropout3(x)

        # Aplana el tensor para pasarlo a la capa fully connected
        x = x.view(-1, self.fc_input_size)

        # Capa densa + ReLU + Dropout
        x = self.dropout_fc(nn.functional.relu(self.fc1(x)))

        # Capa de salida sin activaci√≥n (CrossEntropyLoss lo maneja)
        x = self.fc2(x)
        return x

# ==== Configuraci√≥n ====
# Selecciona GPU si est√° disponible, si no usa CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Instancia el modelo y lo mueve al dispositivo seleccionado
model = Improved3DCNN().to(device)

# Carga el checkpoint entrenado con mejor desempe√±o en test (√©poca 6)
checkpoint = torch.load('/home/michell-alvarez/e_tesisI/EF/model_checkpoint_ef_epoch_test_6.pth', map_location=device)

# Restaura los pesos del modelo desde el checkpoint
model.load_state_dict(checkpoint['model_state_dict'])

# Coloca el modelo en modo evaluaci√≥n (desactiva dropout y batchnorm)
model.eval()

# Define las transformaciones a aplicar a los frames: tensor y normalizaci√≥n
transform = transforms.Compose([
    transforms.ToTensor(),  # Convierte imagen a tensor [C, H, W]
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normaliza al rango [-1, 1]
])

# ==== Funciones ====
# Funci√≥n para preprocesar un video .mp4 y prepararlo para la predicci√≥n
def preprocess_video(video_path):
    # Abre el video con OpenCV
    cap = cv2.VideoCapture(video_path)
    frames = []  # Lista para almacenar los frames procesados

    # Lee los frames uno por uno
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break  # Si no hay m√°s frames, salir del bucle

        # Redimensiona cada frame a 224x224 p√≠xeles
        frame = cv2.resize(frame, (224, 224))

        # Convierte el frame de BGR (OpenCV) a RGB (PyTorch)
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        # Aplica las transformaciones (ToTensor + Normalize)
        frame = transform(frame)

        # Agrega el frame a la lista
        frames.append(frame)

    # Cierra el video
    cap.release()

    # Asegura que haya exactamente 30 frames: rellena con ceros si hay menos
    if len(frames) < 30:
        frames += [torch.zeros(3, 224, 224)] * (30 - len(frames))

    # Si hay m√°s de 30, se queda con los primeros 30
    frames = frames[:30]

    # Convierte la lista de frames en un tensor: [T, C, H, W] ‚Üí [C, T, H, W]
    frames_tensor = torch.stack(frames).permute(1, 0, 2, 3)

    # Agrega dimensi√≥n de batch y mueve al dispositivo
    return frames_tensor.unsqueeze(0).to(device)


# -----------------------------------------------------------
# Funci√≥n para predecir si un tensor de video contiene violencia
# -----------------------------------------------------------
def predict_tensor(video_tensor):
    # Desactiva el c√°lculo de gradientes (mejora el rendimiento en inferencia)
    with torch.no_grad():
        # Realiza la predicci√≥n usando el modelo
        output = model(video_tensor)
        # Obtiene el √≠ndice de la clase con mayor puntuaci√≥n
        _, predicted = torch.max(output.data, 1)
        # Devuelve la etiqueta correspondiente como string
        return "Violencia" if predicted.item() == 1 else "No violencia"

# -----------------------------------------------------------
# Funci√≥n para procesar y predecir todos los videos en una carpeta
# -----------------------------------------------------------
def predict_new_videos(directory):
    resultados = []  # Lista para almacenar los resultados (nombre, predicci√≥n)

    # Itera sobre todos los archivos del directorio
    for video_file in os.listdir(directory):
        # Solo considera archivos .mp4
        if video_file.endswith('.mp4'):
            # Construye la ruta completa del video
            video_path = os.path.join(directory, video_file)

            # Preprocesa el video para convertirlo en un tensor
            video_tensor = preprocess_video(video_path)

            # Realiza la predicci√≥n
            pred = predict_tensor(video_tensor)

            # Almacena el resultado
            resultados.append((video_file, pred))

    # Retorna la lista de predicciones
    return resultados

# -----------------------------------------------------------
# Funci√≥n para recodificar un video a un formato compatible usando ffmpeg
# -----------------------------------------------------------
def recodificar_video(input_path, output_path):
    # Comando ffmpeg para recodificar el video
    comando = [
        "ffmpeg",
        "-i", input_path,            # archivo de entrada
        "-c:v", "libx264",           # codificaci√≥n de video en H.264
        "-preset", "fast",           # preset para velocidad de codificaci√≥n
        "-movflags", "+faststart",   # permite reproducci√≥n progresiva
        "-c:a", "aac",               # codificaci√≥n de audio AAC
        "-b:a", "128k",              # tasa de bits de audio
        output_path                  # archivo de salida
    ]

    # Ejecuta el comando sin mostrar la salida en consola
    subprocess.run(comando, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

# -----------------------------------------------------------
# INTERFAZ GR√ÅFICA CON STREAMLIT
# -----------------------------------------------------------

# T√≠tulo principal de la aplicaci√≥n
st.title("Detecci√≥n de Violencia en Video")

# Men√∫ lateral de navegaci√≥n
with st.sidebar:
    menu = option_menu(
        "Men√∫ principal",  # T√≠tulo del men√∫
        ["Validar videos de Carpeta Local", "Cargar video"],  # Opciones del men√∫
        icons=["folder-check", "cloud-upload"],  # Iconos para cada opci√≥n
        menu_icon="cast",  # Icono del men√∫ principal
        default_index=0,   # √çndice por defecto (opci√≥n seleccionada inicialmente)

        # Estilos personalizados para el men√∫ lateral
        styles={
            "container": {"padding": "5px", "background-color": "#fafafa"},
            "icon": {"color": "black", "font-size": "18px"},
            "nav-link": {
                "font-size": "16px",
                "text-align": "left",
                "margin": "5px",
                "--hover-color": "#eee"
            },
            "nav-link-selected": {"background-color": "#02ab21", "color": "white"},
        }
    )

# -----------------------------------------------------------
# Opci√≥n del men√∫: Validar todos los videos dentro de una carpeta local
# -----------------------------------------------------------
if menu == "Validar videos de Carpeta Local":
    # Encabezado de la secci√≥n
    st.header("üìÅ Validar carpeta local")

    # Ruta fija donde se encuentran los videos a analizar
    carpeta = '/home/michell-alvarez/e_modelos/EF/Violencia/NewData'

    # Si se presiona el bot√≥n de predicci√≥n
    if st.button("Iniciar Predicci√≥n en Carpeta"):
        # Verifica si la carpeta existe
        if os.path.isdir(carpeta):
            # Llama a la funci√≥n de predicci√≥n por carpeta
            resultados = predict_new_videos(carpeta)

            # Muestra mensaje de √©xito
            st.success("Predicci√≥n completada.")

            # Muestra los resultados individualmente
            for archivo, prediccion in resultados:
                # Agrega un emoji seg√∫n la clase predicha
                emoji = "üö®" if prediccion == "Violencia" else "üü¢"
                # Muestra el nombre del video y su predicci√≥n
                st.write(f"üìπ **{archivo}** ‚Üí {emoji} **{prediccion}**")

            # L√≠nea divisoria
            st.markdown("---")

            # Muestra el total de videos analizados
            st.info(f"‚úÖ Total de videos analizados: **{len(resultados)}**")
        else:
            # Mensaje de error si la carpeta no existe
            st.error(f"No se encontr√≥ la carpeta: {carpeta}")

# -----------------------------------------------------------
# Opci√≥n del men√∫: Subir un video individual para an√°lisis
# -----------------------------------------------------------
elif menu == "Cargar video":
    # Encabezado de la secci√≥n
    st.header("üì§ Cargar un video para an√°lisis")

    # Componente para subir archivos .mp4
    uploaded_file = st.file_uploader("Selecciona un archivo .mp4", type=["mp4"])

    # Verifica si se ha subido un archivo
    if uploaded_file is not None:
        # Crea un archivo temporal para guardar el video subido
        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as tmp:
            # Copia el contenido del archivo subido al archivo temporal
            shutil.copyfileobj(uploaded_file, tmp)
            tmp.flush()              # Asegura que se escriba todo en disco
            os.fsync(tmp.fileno())   # Sincroniza con el sistema de archivos
            tmp_path = tmp.name      # Obtiene la ruta al archivo temporal

        # Intenta abrir el video con OpenCV para validarlo
        cap = cv2.VideoCapture(tmp_path)
        if not cap.isOpened():
            # Si el video no puede abrirse, muestra un mensaje de error
            st.error("‚ùå No se pudo abrir el video. El archivo podr√≠a estar da√±ado.")
        else:
            # Obtiene informaci√≥n del video para verificar duraci√≥n
            n_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            duration = n_frames / fps if fps > 0 else 0
            cap.release()

            # Muestra advertencia si el video es muy corto
            if duration < 1:
                st.warning("‚ö†Ô∏è El video tiene muy poca duraci√≥n, puede que no se habilite el bot√≥n de Play.")

            # Recodifica el video para asegurar compatibilidad en Streamlit
            recod_path = tmp_path.replace(".mp4", "_recode.mp4")
            recodificar_video(tmp_path, recod_path)

            # Muestra el video recodificado en la app
            st.video(recod_path)

            # Si el usuario presiona el bot√≥n para predecir el video cargado
            if st.button("Predecir video cargado"):
                # Preprocesa el video y realiza la predicci√≥n
                video_tensor = preprocess_video(recod_path)
                pred = predict_tensor(video_tensor)
                emoji = "üö®" if pred == "Violencia" else "üü¢"
                # Muestra el resultado
                st.success(f"üìπ Video cargado ‚Üí {emoji} **{pred}**")
